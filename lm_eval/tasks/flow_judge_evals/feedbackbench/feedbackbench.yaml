task: feedbackbench
dataset_path: flowaicom/Feedback-Bench

training_split: null
validation_split: null
test_split: "test"

process_docs: !function utils.preprocess_records

doc_to_text: !function utils.format_prompt
doc_to_target: !function utils.format_orig_score

filter_list:
  - name: score_parser
    filter:
      - function: "regex"
        group_select: -1
        regex_pattern: "<score>\\s*(\\d+)\\s*</score>"
      - function: "map"
        mapping_dict:
          "1": 1
          "2": 2
          "3": 3
          "4": 4
          "5": 5
        default_value: "" # To avoid resulting in NoneType error - metrics.py can handle empty strings
      - function: "take_first"

output_type: generate_until
generation_kwargs:
  temperature: 0.1
  top_p: 0.95
  do_sample: true
  
metric_list:
  - metric: !function utils.pearson_corr
    aggregation: !function utils.pearson_corr_agg
    higher_is_better: true
  - metric: !function utils.spearman_corr
    aggregation: !function utils.spearman_corr_agg
    higher_is_better: true
  - metric: !function utils.kendalltau_corr
    aggregation: !function utils.kendalltau_corr_agg
    higher_is_better: true

metadata:
  version: 0.0